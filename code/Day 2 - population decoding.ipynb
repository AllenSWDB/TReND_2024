{"cells":[{"cell_type":"markdown","metadata":{},"source":["![Image](./resources/header.png)\n","<h1 align=\"center\">Decoding data from the Allen Brain Observatory</h1> \n","<h3 align=\"center\">TReND CaMinA 2024</h3>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background: #ADD8E6; border-radius: 3px; padding: 10px;\">\n","\n","In this notebook we're going to continue with the idea of using the neural responses to decode information about the corresponding stimuli, but introducing tools and concepts from machine learning that will simplify this process and let us generalize to decoding an entire population of cells.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### Brain Observatory Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# standard data access code\n","import os\n","import platform\n","from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n","\n","# Set file location based on platform. \n","platstring = platform.platform()\n","if ('Darwin' in platstring) or ('macOS' in platstring):\n","    # macOS \n","    data_root = \"/Volumes/TReND2024/\"\n","elif 'Windows'  in platstring:\n","    # Windows (replace with the drive letter of USB drive)\n","    data_root = \"E:/\"\n","elif ('amzn' in platstring):\n","    # then on Code Ocean\n","    data_root = \"/data/\"\n","else:\n","    # then your own linux platform\n","    # EDIT location where you mounted hard drive\n","    data_root = \"/media/$USERNAME/TReND2024/\"\n","\n","manifest_file = os.path.join(data_root,'allen-brain-observatory/visual-coding-2p/manifest.json')\n","boc = BrainObservatoryCache(manifest_file=manifest_file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# imports for this notebook\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## Load dataset and mean responses"]},{"cell_type":"markdown","metadata":{},"source":["We've chosen a pair of example cells from the same session to work with here. Start by finding the drifting gratings session that contains these cells, and loading the corresponding data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# chosen example cells\n","cell1_id = 517417584\n","cell2_id = 517417136\n","\n","session = \n","\n","data_set = \n","cells_index = \n","\n","# print session information"]},{"cell_type":"markdown","metadata":{},"source":["Next find the code you used yesterday to calculate trial-average responses for all cells in an experiment and apply that here. Drop the blank sweeps from the stim table first, we won't be using those in this analysis."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["One additional preparation step here is that we want to rescale (normalize) the responses by cell, subtracting the mean and dividing by the standard deviation so that all of our features have zero mean and unit variance.\n","\n","Define a `norm_responses` array to store the normalized mean responses, and then also select the subset of sweeps (from the stimulus table) and responses with temporal frequency equal to 2 (we'll come back to other frequencies later.)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Normalize the responses\n","# Tip: this is easier to do with a numpy array\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use the information in the stimulus table to select a subset of responses at a particular temporal frequency\n"]},{"cell_type":"markdown","metadata":{},"source":["## Machine learning with scikit-learn\n","\n","We are going to use some basic machine learning to solve a *classification* problem based on this data: given the responses of a set of cells on a given trial, can we predict the stimulus orientation of that trial? \n","\n","We'll use the package scikit-learn (sklearn), which is a popular and powerful machine learning package in Python. One strength of this package is a simple interface (pattern of use) for a variety of different machine learning tools, or *estimators*. It follows an *object-oriented* approach - each estimator is a Python class which must first be initialized, then passed data to fit and make predictions from.\n","\n","We typically represent the data feature matrix by a variable `X` (capital to indicate a 2D matrix, with each column a distinct feature and each row a sample) and the target data by a variable `y` (lowercase to indicate a 1D vector). Once we have initialized a specific estimator (like `model = LogisticRegression()`), the key methods always take the same form: we fit the model using `model.fit(X, y)` and make predictions using `y_pred = model.predict(X)`.\n","\n","This pattern can be used for different types of problems: *classification* when the target is categorical, *regression* when the target is continuous, or even *unsupervised* problems like dimensionality reduction or clustering where there is no target data to train on. Chains of estimators can also be combined into *pipelines* that can be used just as easily as any other estimator.\n"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n","\n","<h1>Part 1: Making and interpreting predictions from 2 cells</h1>\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### Binary classification"]},{"cell_type":"markdown","metadata":{},"source":["\n","We'll start with a *binary classification* of the drifting grating stimuli, predicting whether the orientation matches a single target orientation, based on mean responses from just a single cell."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# First define X and y\n","\n","# Create a binary indicator for our selected orientation\n","target_orientation = 45\n","\n","# and select the response of the first cell, making sure it has shape (n_trials, 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an instance of the Logistic Regression model\n","# we've filled in some parameters that are needed for this particular problem\n","from sklearn.linear_model import LogisticRegression\n","model = LogisticRegression(class_weight=\"balanced\", C=1e6)\n","\n","# Fit the model\n","\n","\n","# Predict the target variable \n","\n","\n","# Calculate the accuracy of the model (on what percent of trials does the prediction match the target)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizing classifier predictions\n","The classifier we chose has the advantage of a fairly simple mathematical formulation, allowing us to interpret how it makes its predictions. It is a *linear* classifier, meaning the classifier predicts \"true\" if $C + k \\cdot x > 0$ and \"false\" otherwise, defining a dividing line to split the input space into true and false samples. We can plot this line using properties of the fitted classifier."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# first create a scatter plot of the responses and target variable (y vs X)\n","# with 'true' responses in red and 'false' in blue\n","\n","\n","# pull the coefficient k and intercept C out of the classifier\n","k = model.coef_[0][0]\n","C = model.intercept_\n","\n","# calculate the dividing line and plot with axvline\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Logistic regression and probabilities\n","The parameters of this line are determined by optimizing a probabilistic model of the data, $\\mathrm{Pr}(y) = f(C + k_0 \\cdot x_0 + k_1 \\cdot x_1 + \\dots)$, where $f$ is the *logistic function* shown in the plot below, $f(x) = \\frac{1}{1-e^{-x}}$:\n","\n","![logistic curve](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n","\n","We can check these probabilities for any point in our input space using the function `model.predict_proba(X)`. We can then add probabilities to our two-cell response plot also."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create the plot again...\n","\n","\n","# define a range of x-values to calculate probabilities for\n","\n","\n","# predict_proba returns two columns, one for each class - we want the probability of class 1 (angle=45)\n","# Tip: [:,None] reshapes the array to have shape (n, 1) instead of (n,)\n","probs = model.predict_proba(xx[:,None])[:, 1]\n","\n","# plot the probabilities"]},{"cell_type":"markdown","metadata":{},"source":["### Classification in 2 dimensions\n","\n","We can add a second cell and fit as before, then visualize this case also, in two dimensions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# redefine X to include both cells\n","\n","\n","# Fit the model\n","\n","\n","# find the accuracy - try using model.score instead of calculating it manually\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# There is a built-in function to visualize the classifier boundary and probabilities\n","from sklearn.inspection import DecisionBoundaryDisplay\n","\n","# we've filled in some suggested parameters here, but feel free to change them\n","DecisionBoundaryDisplay.from_estimator(\n","    model,\n","    X,\n","    plot_method=\"pcolormesh\",\n","    response_method=\"predict_proba\",\n","    ax=plt.gca(),\n","    cmap=\"coolwarm\",\n","    vmin=0,\n","    vmax=1,\n","    alpha=0.5,\n","    shading=\"auto\",\n","    eps=0.01,\n",")\n","\n","# add the data points as before\n","\n","# add a colorbar showing the scale\n","plt.colorbar(label=f\"Probability of angle={target_orientation}\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Extra: Detailed walkthrough of 2D visualization methods"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# first create a scatter plot of the two cells' responses colored by y\n","# with 'true' responses in red and 'false' in blue\n","cell1_responses = selected_responses[:, cells_index[0]]\n","cell2_responses = selected_responses[:, cells_index[1]]\n","target_trials = y.astype(bool)\n","plt.scatter(cell1_responses[target_trials], cell2_responses[target_trials], s=10, color='red', label=f\"angle={target_orientation}\")\n","plt.scatter(cell1_responses[~target_trials], cell2_responses[~target_trials], s=10, color='blue', label=\"other angles\")\n","plt.xlabel(f'Cell {cells_index[0]} Response')\n","plt.ylabel(f'Cell {cells_index[1]} Response')\n","\n","\n","# pull the coefficient parameters k out of the classifier\n","k = model.coef_[0]\n","C = model.intercept_\n","\n","# pick evenly spaced x-values to plot the classifier boundary line\n","n = 10\n","x_line = np.linspace(cell1_responses.min(), cell1_responses.max(), n)\n","\n","# calculate the y-values of the classifier boundary line using the equation above\n","y_line = (-C - k[0] * x_line) / k[1]\n","plt.plot(x_line, y_line, ':', label=\"classifier boundary\")\n","\n","# adjust the plot bounds with plt.ylim\n","plt.ylim(cell2_responses.min(), cell2_responses.max())\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# the coolwarm colormap is nice for probabilities, mapping values between 0 and 1 to colors from blue to red\n","cmap = plt.cm.coolwarm\n","# we can use this colormap to color our response scatterplot by the binary targets y\n","\n","# pass evenly spaced x and y coordinates to np.meshgrid to create a grid\n","n = 50\n","xx = np.linspace(cell1_responses.min(), cell1_responses.max(), n)\n","yy = np.linspace(cell2_responses.min(), cell2_responses.max(), n)\n","xgrid, ygrid = np.meshgrid(xx, yy)\n","\n","# flatten and stack these lists to create a X matrix to pass to model.predict_proba\n","Xgrid = np.vstack([xgrid.flatten(), ygrid.flatten()]).T\n","probs = model.predict_proba(Xgrid)[:, 1]\n","\n","# reshape the probabilities to fit the shape of the grid\n","probs_grid = probs.reshape(n, n)\n","plt.pcolormesh(xgrid, ygrid, probs_grid, cmap=cmap, alpha=0.5, shading='auto')\n","\n","# finally, copy the code for our response scatter plot to overlay it here\n","plt.scatter(cell1_responses, cell2_responses, s=10, c=y, cmap='coolwarm')\n","plt.plot(x_line, y_line, ':', label=\"classifier boundary\")\n","plt.xlabel(f'Cell {cells_index[0]} Response')\n","plt.ylabel(f'Cell {cells_index[1]} Response')\n","\n","# and add a colorbar for the probabilities\n","plt.colorbar(label=f'Probability of angle={target_orientation}')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n","\n","<h1>Part 2: Population decoding</h1>\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### Predicting all orientations\n","\n","We can also use the `LogisticRegression` classifier to predict which of the 8 orientations a response comes from. This involves a mathematical model for the probability of each class/orientation, which are then compared for a final prediction. Scikit-learn makes this very easy, adapting the model to do this in the background if we simply pass a *categorical* rather than *binary* target variable `y`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# redefine y to include all orientations, fit, and find the model accuracy score\n","# Hint: you can actually use the column directly from the dataframe (or convert to an array)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Here's a way to quickly plot the responses labeled by orientation,\n","# to assess whether this poor performance makes sense\n","plt.scatter(*X.T, s=10, c=y, cmap='tab10')\n","plt.xlabel(f'Cell {cells_index[0]} Response')\n","plt.ylabel(f'Cell {cells_index[1]} Response')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["This is not very good, as expected - two cells can only tell us so much. However, sklearn makes it very easy to train on all recorded cells in the session together!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We need to adjust the model here for this more complex setting\n","model = LogisticRegression(max_iter=10000)\n","\n","# redefine X to include all cells, fit, and find the model accuracy score\n"]},{"cell_type":"markdown","metadata":{},"source":["Discuss: Does anything seem odd to you about this result? Let's discuss why this may not be as perfect as it seems."]},{"cell_type":"markdown","metadata":{},"source":["### Splitting training and test data\n","\n","Our ultimate goal is a model that captures **true** patterns in the data, but what does \"true\" mean here? One way of resolving this scientifically would be to experiment: generate more data from the same cells and see if the same patterns are present (if the estimator can predict the stimuli well on the new data). Practically, we can't usually repeat experiments like this, but we can instead hide some of the existing data during the *fit* step and use it only to predict and evaluate. We call the fit data the training dataset, and the hidden data the test dataset.\n","\n","Sklearn has a method to shuffle the data (across trials) and make this split randomly `train_test_split`.\n","\n","Discuss: Why might shuffling be important here?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# this introduces randomness, so set the random 'seed' for reproducibility\n","np.random.seed(5)\n","# stratify=y ensures that the train and test sets have the same proportion of each class\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model on the training data, and score (predict) on the test data\n"]},{"cell_type":"markdown","metadata":{},"source":["### Viewing the confusion matrix"]},{"cell_type":"markdown","metadata":{},"source":["Let's also visualize a confusion matrix of the pattern of errors. Sklearn has a built-in method for this - try out the function `sklearn.metrics.ConfusionMatrixDisplay.from_estimator()`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n"]},{"cell_type":"markdown","metadata":{},"source":["### Fitting with all temporal frequencies\n","\n","It's hard to tell if there is a pattern in the above confusion matrix because there are not many trials to plot in our test set. Let's try adding back in the other trials."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define X and y, fit and score the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot a confusion matrix\n"]},{"cell_type":"markdown","metadata":{},"source":["In this case, decoding performance is similar when we include all temporal frequencies. This may not always be true: the effect of temporal frequency can be very different in different experiments if different cell types are sampled (by Cre or depth targeting, or just different fields of view).\n","\n","One advantage of the full dataset, though, is that with more samples we can begin to see a clear pattern to the errors in the confusion matrix plot.\n","\n","Discuss: what do you think this pattern means?"]},{"cell_type":"markdown","metadata":{},"source":["# Exercises:"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: Train a classifier on natural image responses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# find the dataset and stim table for natural scenes for the same cells as above\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# how many times is each stimulus presented?\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# choose a handful of images and subset the stim table to just the relevant trials\n","images = [13, 15, 18]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for fun, let's plot the images we've selected (see examples from yesterday)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# make your code from above into a function to calculate the normalized mean sweep response\n","def get_mean_sweep_response(dff, stim_table):\n","    # fill\n","    return norm_responses\n","\n","# then run it on the natural scenes data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define X and y for responses of the pair of cells,\n","# then plot the responses colored by the image frame\n","\n","# Tip: the cell indices may be different in this new session/dataset\n","\n","\n","# don't forget to add a legend (and labels)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define X and y for the population of cells, split train and test data,\n","# then fit and score a logistic regression classifier and plot the confusion matrix\n","model = LogisticRegression(max_iter=10000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: training with \"class imbalance\"\n","\n","You might be wondering why we set the option `LogisticRegression(class_weight='balanced')`. Try rerunning the binary classification portions without this option and see if you can make sense of how the predictions and classifier boundary change. Hint: it's related to the fact that there are many more \"false\" trials than \"true\" trials. \n","\n","While the accuracy score will likely improve, you should be able to visually identify why we might not want this result. There are other metrics in the `sklearn.metrics` module that may better capture our intuitive idea of a \"good model\" here - try `f1_score` for instance."]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: Scoring with cross validation\n","\n","*Cross-validation* is an approach to test model performance on the entire dataset, while avoiding the problem of training and testing on the same data. Instead, the data is split multiple times (with the model retrained each time), so that each sample is part of the test set a single time.\n","\n","![diagram of cross-validation](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/640px-K-fold_cross_validation_EN.svg.png)\n","\n","Scikit-learn contains a function that makes this ease, `cross_val_score` - try using this to compare performance on all temporal frequencies vs the single one we selected. These results should now be more reliable across multiple runs than our previous comparison!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","model = LogisticRegression(max_iter=10000)\n","\n","# run cross_val_score on all sweeps, predicting all orientations\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run cross_val_score on selected sweeps (single TF)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: Classifier for both TF & orientation\n","\n","We've seen that decoding can do fairly well with all the data, not knowing about the temporal frequency at all. Is it possible we could increase performance even more by training on both orientation and temporal frequency labels together? A few classifiers in sklearn can function in the *multi-output* context where we pass a 2D matrix of target data instead of a vector. We'll test out one of these, the random forest classifier."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Define the target variable and the feature matrix, and run the train_test_split function\n","\n","# Create an instance of the Random Forest model, fit the model, and predict the target variable\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we can't use model.score in this context, so instead generate a prediction of the target variables,\n","# and evaluate accuracy separately each one, using the accuracy_score function\n","from sklearn.metrics import accuracy_score\n"]},{"cell_type":"markdown","metadata":{},"source":["This is similar to before, but is it actually better? How much does performance vary run to run? As implied in the name, the random forest classifier has an element of randomness in its performance.\n","\n","We should also compare to the same classifier's performance on orientation data only."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Repeat the above process with orientation data only in Y\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":4}
